{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "> 1. Convert all the sentences to lower case\n",
    "> 2. Remove punctuations and special characters\n",
    "> 3. Remove Stopwords (both language specifi and custom)\n",
    "> 4. Perform Tokenization\n",
    "> 5. Perform Stemming (Snowball Stemmer being used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = SnowballStemmer('english') # Load the SnoballStemmer\n",
    "default_stopwords = set(stopwords.words('english')) # default stopwords of English Language\n",
    "custom_stopwords = ['naga'] #Custom Stopword list usecase specific, ensure that you enter data in lower case\n",
    "final_stopwords = default_stopwords.union(custom_stopwords) # Final Stopword list combination of default and custom\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    filtered_words=[]\n",
    "    for word in words:\n",
    "        if word not in final_stopwords:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "def stem_words(words):\n",
    "    stemmed_words=[]\n",
    "    for word in words:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    return stemmed_words\n",
    "    \n",
    "def tokenize(sentence):\n",
    "    sentence=sentence.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens=nltk.word_tokenize(sentence)\n",
    "    filtered_words=filter_stopwords(tokens)\n",
    "    stemmed_words=stem_words(filtered_words)\n",
    "    return stemmed_words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "> 1. Transform input data to Document Term Matrix (DTM)\n",
    "> 2. Construct Bag of Words Model (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_features(data, custom_tokenizer=tokenize, verbose=False):\n",
    "    count_vectorizer = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "    print(count_vectorizer.fit_transform(data).todense())\n",
    "    if True==verbose:\n",
    "        print('\\nFeature Extraction Summary ')\n",
    "        print('###########################')\n",
    "        print('Vocabulary: ')\n",
    "        print(count_vectorizer.get_feature_names())\n",
    "        print('Feature Vector Dimesion: ',len(count_vectorizer.get_feature_names()))\n",
    "    return count_vectorizer,ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x11028af28>, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "test_set=[]\n",
    "test_set.append('Hello')\n",
    "test_set.append('hi naga how are you man!!, What is your favourite number? Is it #9?')\n",
    "ct,countv=extract_features(test_set)\n",
    "print(ct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
